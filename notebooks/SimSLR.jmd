
## Simulation of simple linear regression estimates

To illustrate some of the capabilities of `DrWatson`for simulations we consider a simple example of simulating the parameter estimates from a simple linear regression model.

In the second part of the workshop we will consider data from an experiment on the effects of sleep deprivation on reaction time.
For each subject in the study we will assume a simple linear regression model, 
$$y_i = a*x_i + b + \epsilon_i,\; i = 1,\dots,10$$
for their $i$th reaction time, $y_i$, as a function of the number of days of sleep deprivation, $x_i$.

For subject `S334` the estimates of $a$ and $b$ are approximately $a=12.25$ ms/day and $b=240.16$ ms.
An estimate of the residual standard deviation is $s=8.55$ ms, which we will use as the value of $\sigma$ in our simulation.

For all the subjects, the days of sleep deprivation are from 0 to 9.

## Parameter tuples

These parameter values are incorporated into a _named tuple_

```julia
pars = (a = 12.25, b = 240.16, σ = 8.55, x = 0:9)
```

```julia
typeof(pars)
```

A Julia `NamedTuple` is similar to a named list in R except that the names are _symbols_ (the symbol 'a' is written ':a') and the types of the values are part of the tuple type itself.
In `DrWatson` a simulation function typically takes a parameter tuple which is immediately _unpacked_ by calling the `@unpack` macro.
A parameter tuple can also be used to generate a meaningful file name under which to save the results of a simulation.

```julia
using DrWatson, LinearAlgebra, Random, PrettyTables
```

```julia
savename(pars, "arrow")  # file name for an Arrow file of results from these parameters
```

## Generating a response and estimating parameters

A simple function to generate a response is

```julia
function oneresp(pars)
    @unpack a, b, σ, x = pars  # expand the name/value pairs
    a .* x .+ b .+ σ * randn(length(x))
end
```

```julia
oneresp(pars)
```

A call to `randn(n)` generates an i.i.d. sample of size `n` from a standard normal distribution.
We scale these values by σ and add this "noise" to the assumed "true" response, `a * x + b`.

In Julia the "dots" ('.') before an operator like '+' or '\*' cause it to be broadcast over arrays.

### Reproducible "random" responses

It is often convenient (like when you are debugging) to be able to reproduce the "random" numbers that were generated.
To allow for this we will define another `oneresp` method that allows for a random number generator to be passed to it. 

```julia
function oneresp(rng::AbstractRNG, pars::NamedTuple)
    @unpack a, b, σ, x = pars
    a .* x .+ b .+ σ .* randn(rng, length(x))
end
```

```julia
rng = MersenneTwister(42);   # initialize a random number generator
```

```julia
oneresp(rng, pars)
```

```julia
y = oneresp(MersenneTwister(42), pars)  # reproduce those results
```

### Estimates from a single sample

The easiest way to obtain the least squares estimates is by building the model matrix and using the "backslash" operator, '\'.

```julia
X = hcat(ones(10), 0:9)
```

```julia
coef = X \ y
```

And $s^2$, the estimated residual variance, is

```julia
s² = sum(abs2, y .- X * coef) / (size(X, 1) - size(X, 2))
```

(`abs2(x)` returns `x*x` when `x` is a real (as in, non-complex) number.)

### Simulating N parameter estimates

At this point we can extend `oneresp` to do the calculations and return a NamedTuple

```julia
function onepars(rng::AbstractRNG, pars::NamedTuple)
    @unpack a, b, σ, x = pars
    X = hcat(ones(length(x)), x)
    n, p = size(X)
    y = a .* x .+ b .+ σ .* randn(rng, n)
    coef = X \ y
    (slope = last(coef), intercept = first(coef), s² = sum(abs2, y .- X * coef)/(n - p))
end
```

```julia
onepars(MersenneTwister(42), pars)
```

To generate a sample of these parameter estimates we first initialize a random number generator

```julia
rng = MersenneTwister(6354789);
samp = [onepars(rng, pars) for i in 1:100]
```

It may seem like a waste to repeat the names for each result but, in fact, the names are only stored once for this vector.

```julia
typeof(samp)
```

A vector of `NamedTuple`s is a row-oriented `Table`, as defined inthe `Tables` package.
A column-oriented table, such as a `DataFrame`, could be a `NamedTuple` of vectors.
Both types can be shown with `pretty_table` from the `PrettyTables` package.

```julia
pretty_table(samp)
```

## Tuning the simulation

So, this approach works and produces simulated values that behave as expected.

However, we are repeating a lot of effort for each evaluation of the simulation loop.
At each iteration we create the same model matrix (of the same size) and the fixed part of the response, $\mathbf{y}$.
And in the background the expression `X \ y` will copy `X` and `y` and decompose the copy of `X` into a "QR" decomposition.

As is true in many languages, we can avoid many of these steps with enough effort in Julia.
However, unlike other languages like R or Python, we do not need to "drop down" to a compiled language like C or C++ to take advantage of the low-level features in Julia.
The magic of Julia is that the language offers you such a wide range of capabilities from high level to very low level using the same tools throughout.

An optimized version of a simulation function could look like

```julia
function simslr(rng::AbstractRNG, pars, N)
    @unpack a, b, σ, x = pars
    n = length(x)
    qrfac = qr!(hcat(ones(length(x)), x))
    Qt = qrfac.Q'
    R = UpperTriangular(qrfac.R)
    y₀ = convert(Vector{eltype(qrfac)}, a .* x .+ b)
    y = similar(y₀)
    cv = view(y, 1:2)   # view of the part of Q'y defining the coefficients
    rv = view(y, 3:n)   # view of the part of Q'y defining the residuals
    map(1:N) do i
        for j in axes(y, 1)
            y[j] = y₀[j] + σ * randn(rng)
        end
        lmul!(Qt, y)
        ldiv!(R, cv)
        (slope = y[2], intercept = y[1], s² = sum(abs2, rv)/(n - 2))
    end
end
```

To ensure that it works

```julia
rng = MersenneTwister(6354789);
samp2 = simslr(rng, pars, 100);
pretty_table(samp2)
```

A superficial comparison shows that the two samples are similar.
A more detailed comparison, say by converting each of them to `DataFrame`s and using `isapprox`, would show they are the same samples.

However, their execution times and the amount of storage used when generating, say, 10,000 samples are very different.

```julia
rng = MersenneTwister(6354789)
@time [onepars(rng, pars) for i in 1:10_000];
```

```julia
rng = MersenneTwister(6354789)
@time simslr(rng, pars, 10_000);
```

The optimized version is over 10 times faster and requires much less memory and fewer allocations than the original version.

The savings in memory allocations and usage come from the fact that all of the memory to be used, except for the storage of the result itself, is allocated before entering the simulation loop and re-used within the loop.
Unlike R, Julia allows a function to modify its arguments if they are composite structures like vectors.
It is customary to append "!" to the names of such _mutating_ functions, like `lmul!` (multiply, in-place, on the left) and `ldiv!` (divide, in-place, on the left).

Also, in the simulation loop itself, the vector `y` is filled in with the fixed part of the response plus the random noise in an inner loop.
This is the exact opposite of the programming style favored in R and Python where loops are avoided at all cost.
In Julia a loop is a perfectly fine way of programming an iterative operation; there is no need to perform contorsions to "vectorize" operations.
