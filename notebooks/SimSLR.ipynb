{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of simple linear regression estimates\n",
    "\n",
    "To illustrate some of the capabilities of `DrWatson`for simulations we consider a simple example of simulating the parameter estimates from a simple linear regression model.\n",
    "\n",
    "In the second part of the workshop we will consider data from an experiment on the effects of sleep deprivation on reaction time.\n",
    "For each subject in the study we will assume a simple linear regression model, \n",
    "$$y_i = a*x_i + b + \\epsilon_i,\\; i = 1,\\dots,10$$\n",
    "for their $i$th reaction time, $y_i$, as a function of the number of days of sleep deprivation, $x_i$.\n",
    "\n",
    "For subject `S334` the estimates of $a$ and $b$ are approximately $a=12.25$ ms/day and $b=240.16$ ms.\n",
    "An estimate of the residual standard deviation is $s=8.55$ ms, which we will use as the value of $\\sigma$ in our simulation.\n",
    "\n",
    "For all the subjects, the days of sleep deprivation are from 0 to 9.\n",
    "\n",
    "## Parameter tuples\n",
    "\n",
    "These parameter values are incorporated into a _named tuple_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(a = 12.25, b = 240.16, σ = 8.55, x = 0:9)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "pars = (a = 12.25, b = 240.16, σ = 8.55, x = 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NamedTuple{(:a, :b, :σ, :x),Tuple{Float64,Float64,Float64,UnitRange{Int64}}}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "typeof(pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Julia `NamedTuple` is similar to a named list in R except that the names are _symbols_ (the symbol 'a' is written ':a') and the types of the values are part of the tuple type itself.\n",
    "In `DrWatson` a simulation function typically takes a parameter tuple which is immediately _unpacked_ by calling the `@unpack` macro.\n",
    "A parameter tuple can also be used to generate a meaningful file name under which to save the results of a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DrWatson, LinearAlgebra, Random, PrettyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"a=12.25_b=240.16_σ=8.55.arrow\""
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "savename(pars, \"arrow\")  # file name for an Arrow file of results from these parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a response and estimating parameters\n",
    "\n",
    "A simple function to generate a response is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "oneresp (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "function oneresp(pars)\n",
    "    @unpack a, b, σ, x = pars  # expand the name/value pairs\n",
    "    a .* x .+ b .+ σ * randn(length(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 235.07952500038138\n",
       " 241.51647464169255\n",
       " 258.4024107819031\n",
       " 267.7856435280689\n",
       " 265.95680562786606\n",
       " 300.8163593197814\n",
       " 321.7553256948827\n",
       " 336.3317163157975\n",
       " 339.2794896297591\n",
       " 344.53906645099346"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "oneresp(pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to `randn(n)` generates an i.i.d. sample of size `n` from a standard normal distribution.\n",
    "We scale these values by σ and add this \"noise\" to the assumed \"true\" response, `a * x + b`.\n",
    "\n",
    "In Julia the \"dots\" ('.') before an operator like '+' or '\\*' cause it to be broadcast over arrays.\n",
    "\n",
    "### Reproducible \"random\" responses\n",
    "\n",
    "It is often convenient (like when you are debugging) to be able to reproduce the \"random\" numbers that were generated.\n",
    "To allow for this we will define another `oneresp` method that allows for a random number generator to be passed to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "oneresp (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "function oneresp(rng::AbstractRNG, pars::NamedTuple)\n",
    "    @unpack a, b, σ, x = pars\n",
    "    a .* x .+ b .+ σ .* randn(rng, length(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(42);   # initialize a random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 235.4059702089484\n",
       " 248.6105222967121\n",
       " 264.89217813997857\n",
       " 274.3494110274313\n",
       " 304.3607123883901\n",
       " 291.62109190371854\n",
       " 309.65341970746636\n",
       " 327.2450266055783\n",
       " 315.57097688094336\n",
       " 358.98829965747854"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "oneresp(rng, pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 235.4059702089484\n",
       " 248.6105222967121\n",
       " 264.89217813997857\n",
       " 274.3494110274313\n",
       " 304.3607123883901\n",
       " 291.62109190371854\n",
       " 309.65341970746636\n",
       " 327.2450266055783\n",
       " 315.57097688094336\n",
       " 358.98829965747854"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y = oneresp(MersenneTwister(42), pars)  # reproduce those results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimates from a single sample\n",
    "\n",
    "The easiest way to obtain the least squares estimates is by building the model matrix and using the \"backslash\" operator, '\\'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10×2 Array{Float64,2}:\n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  3.0\n",
       " 1.0  4.0\n",
       " 1.0  5.0\n",
       " 1.0  6.0\n",
       " 1.0  7.0\n",
       " 1.0  8.0\n",
       " 1.0  9.0"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X = hcat(ones(10), 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 238.90883010866935\n",
       "  12.03576239399893"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "coef = X \\ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $s^2$, the estimated residual variance, is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "114.91981195054993"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "s² = sum(abs2, y .- X * coef) / (size(X, 1) - size(X, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`abs2(x)` returns `x*x` when `x` is a real (as in, non-complex) number.)\n",
    "\n",
    "### Simulating N parameter estimates\n",
    "\n",
    "At this point we can extend `oneresp` to do the calculations and return a NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "onepars (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "function onepars(rng::AbstractRNG, pars::NamedTuple)\n",
    "    @unpack a, b, σ, x = pars\n",
    "    X = hcat(ones(length(x)), x)\n",
    "    n, p = size(X)\n",
    "    y = a .* x .+ b .+ σ .* randn(rng, n)\n",
    "    coef = X \\ y\n",
    "    (slope = last(coef), intercept = first(coef), s² = sum(abs2, y .- X * coef)/(n - p))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(slope = 12.03576239399893, intercept = 238.90883010866935, s² = 114.91981195054993)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "onepars(MersenneTwister(42), pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a sample of these parameter estimates we first initialize a random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100-element Array{NamedTuple{(:slope, :intercept, :s²),Tuple{Float64,Float64,Float64}},1}:\n",
       " (slope = 11.239335089889355, intercept = 242.8177243543006, s² = 27.864399149440857)\n",
       " (slope = 11.970257146559899, intercept = 241.32913529965157, s² = 73.3114146323817)\n",
       " (slope = 13.639242254929487, intercept = 229.61345586372468, s² = 106.70650295502332)\n",
       " (slope = 12.900776376672322, intercept = 237.82729770208044, s² = 41.95990076259141)\n",
       " (slope = 15.24199627168337, intercept = 224.92389520814814, s² = 112.80185067488046)\n",
       " (slope = 12.811584357688236, intercept = 240.75844952653952, s² = 64.55037032600164)\n",
       " (slope = 11.970077253742533, intercept = 242.90599134310474, s² = 96.49547771466077)\n",
       " (slope = 11.34362793800242, intercept = 244.7740356685669, s² = 55.44348721003346)\n",
       " (slope = 11.877827287849142, intercept = 241.62964082235123, s² = 76.55214559549064)\n",
       " (slope = 12.807874932977553, intercept = 236.18500111592968, s² = 19.08588966027339)\n",
       " (slope = 11.987420524345353, intercept = 241.7533764108515, s² = 257.6549004675872)\n",
       " (slope = 12.354586750603177, intercept = 239.12047342509481, s² = 76.73287567482483)\n",
       " (slope = 10.558320092872055, intercept = 246.6964322491987, s² = 48.702487149836934)\n",
       " ⋮\n",
       " (slope = 12.049109392183224, intercept = 240.76493842866986, s² = 49.26845160992325)\n",
       " (slope = 12.608173609497692, intercept = 236.63768032467988, s² = 52.81559708124648)\n",
       " (slope = 13.114206918795157, intercept = 235.70914858380684, s² = 47.775879519118895)\n",
       " (slope = 12.983132476160817, intercept = 241.34463525371686, s² = 147.615966312941)\n",
       " (slope = 13.408209298671915, intercept = 234.5162153275074, s² = 104.3819210998212)\n",
       " (slope = 13.181271267335244, intercept = 230.88156280904843, s² = 96.74994868248824)\n",
       " (slope = 13.663212793181382, intercept = 233.74542382051985, s² = 33.06333228511331)\n",
       " (slope = 12.271708778590035, intercept = 242.28116651189197, s² = 144.3483403902128)\n",
       " (slope = 12.377196666311837, intercept = 240.69047383479148, s² = 102.05653987374319)\n",
       " (slope = 9.688356764779648, intercept = 251.00569343090257, s² = 36.874278670184964)\n",
       " (slope = 12.017334598775749, intercept = 240.88350193804789, s² = 63.399267661812225)\n",
       " (slope = 12.312621920068413, intercept = 240.71406637380173, s² = 58.172228504966505)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "rng = MersenneTwister(6354789);\n",
    "samp = [onepars(rng, pars) for i in 1:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem like a waste to repeat the names for each result but, in fact, the names are only stored once for this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Array{NamedTuple{(:slope, :intercept, :s²),Tuple{Float64,Float64,Float64}},1}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "typeof(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector of `NamedTuple`s is a row-oriented `Table`, as defined inthe `Tables` package.\n",
    "A column-oriented table, such as a `DataFrame`, could be a `NamedTuple` of vectors.\n",
    "Both types can be shown with `pretty_table` from the `PrettyTables` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "┌─────────┬───────────┬─────────┐\n│\u001b[1m   slope \u001b[0m│\u001b[1m intercept \u001b[0m│\u001b[1m      s² \u001b[0m│\n│\u001b[90m Float64 \u001b[0m│\u001b[90m   Float64 \u001b[0m│\u001b[90m Float64 \u001b[0m│\n├─────────┼───────────┼─────────┤\n│ 11.2393 │   242.818 │ 27.8644 │\n│ 11.9703 │   241.329 │ 73.3114 │\n│ 13.6392 │   229.613 │ 106.707 │\n│ 12.9008 │   237.827 │ 41.9599 │\n│  15.242 │   224.924 │ 112.802 │\n│ 12.8116 │   240.758 │ 64.5504 │\n│ 11.9701 │   242.906 │ 96.4955 │\n│ 11.3436 │   244.774 │ 55.4435 │\n│ 11.8778 │    241.63 │ 76.5521 │\n│ 12.8079 │   236.185 │ 19.0859 │\n│ 11.9874 │   241.753 │ 257.655 │\n│ 12.3546 │    239.12 │ 76.7329 │\n│ 10.5583 │   246.696 │ 48.7025 │\n│ 12.7938 │    230.55 │ 104.929 │\n│ 12.8718 │   237.081 │ 70.3809 │\n│ 12.1422 │   245.316 │  50.597 │\n│ 11.1573 │   244.447 │ 73.3371 │\n│ 12.4879 │   241.083 │ 63.2412 │\n│ 12.6231 │   235.876 │ 125.384 │\n│ 12.7489 │    239.39 │ 29.3548 │\n│  12.974 │   236.665 │ 72.9049 │\n│    ⋮    │     ⋮     │    ⋮    │\n└─────────┴───────────┴─────────┘\n\u001b[31m                  79 rows omitted\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_table(samp)"
   ]
  },
  {
   "source": [
    "## Tuning the simulation\n",
    "\n",
    "So, this approach works and produces simulated values that behave as expected.\n",
    "\n",
    "However, we are repeating a lot of effort for each evaluation of the simulation loop.\n",
    "At each iteration we create the same model matrix (of the same size) and the fixed part of the response, $\\mathbf{y}$.\n",
    "And in the background the expression `X \\ y` will copy `X` and `y` and decompose the copy of `X` into a \"QR\" decomposition.\n",
    "\n",
    "As is true in many languages, we can avoid many of these steps with enough effort in Julia.\n",
    "However, unlike other languages like R or Python, we do not need to \"drop down\" to a compiled language like C or C++ to take advantage of the low-level features in Julia.\n",
    "The magic of Julia is that the language offers you such a wide range of capabilities from high level to very low level using the same tools throughout.\n",
    "\n",
    "An optimized version of a simulation function could look like"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "simslr (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "function simslr(rng::AbstractRNG, pars, N)\n",
    "    @unpack a, b, σ, x = pars\n",
    "    n = length(x)\n",
    "    qrfac = qr!(hcat(ones(length(x)), x))\n",
    "    Qt = qrfac.Q'\n",
    "    R = UpperTriangular(qrfac.R)\n",
    "    y₀ = convert(Vector{eltype(qrfac)}, a .* x .+ b)\n",
    "    y = similar(y₀)\n",
    "    cv = view(y, 1:2)   # view of the part of Q'y defining the coefficients\n",
    "    rv = view(y, 3:n)   # view of the part of Q'y defining the residuals\n",
    "    map(1:N) do i\n",
    "        for j in axes(y, 1)\n",
    "            y[j] = y₀[j] + σ * randn(rng)\n",
    "        end\n",
    "        lmul!(Qt, y)\n",
    "        ldiv!(R, cv)\n",
    "        (slope = y[2], intercept = y[1], s² = sum(abs2, rv)/(n - 2))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "source": [
    "To ensure that it works"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "┌─────────┬───────────┬─────────┐\n│\u001b[1m   slope \u001b[0m│\u001b[1m intercept \u001b[0m│\u001b[1m      s² \u001b[0m│\n│\u001b[90m Float64 \u001b[0m│\u001b[90m   Float64 \u001b[0m│\u001b[90m Float64 \u001b[0m│\n├─────────┼───────────┼─────────┤\n│ 11.2393 │   242.818 │ 27.8644 │\n│ 11.9703 │   241.329 │ 73.3114 │\n│ 13.6392 │   229.613 │ 106.707 │\n│ 12.9008 │   237.827 │ 41.9599 │\n│  15.242 │   224.924 │ 112.802 │\n│ 12.8116 │   240.758 │ 64.5504 │\n│ 11.9701 │   242.906 │ 96.4955 │\n│ 11.3436 │   244.774 │ 55.4435 │\n│ 11.8778 │    241.63 │ 76.5521 │\n│ 12.8079 │   236.185 │ 19.0859 │\n│ 11.9874 │   241.753 │ 257.655 │\n│ 12.3546 │    239.12 │ 76.7329 │\n│ 10.5583 │   246.696 │ 48.7025 │\n│ 12.7938 │    230.55 │ 104.929 │\n│ 12.8718 │   237.081 │ 70.3809 │\n│ 12.1422 │   245.316 │  50.597 │\n│ 11.1573 │   244.447 │ 73.3371 │\n│ 12.4879 │   241.083 │ 63.2412 │\n│ 12.6231 │   235.876 │ 125.384 │\n│ 12.7489 │    239.39 │ 29.3548 │\n│  12.974 │   236.665 │ 72.9049 │\n│    ⋮    │     ⋮     │    ⋮    │\n└─────────┴───────────┴─────────┘\n\u001b[31m                  79 rows omitted\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rng = MersenneTwister(6354789);\n",
    "samp2 = simslr(rng, pars, 100);\n",
    "pretty_table(samp2)"
   ]
  },
  {
   "source": [
    "A superficial comparison shows that the two samples are similar.\n",
    "A more detailed comparison, say by converting each of them to `DataFrame`s and using `isapprox`, would show they are the same samples.\n",
    "\n",
    "However, their execution times and the amount of storage used when generating, say, 10,000 samples are very different."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0.095813 seconds (488.83 k allocations: 41.115 MiB, 10.72% gc time)\n"
     ]
    }
   ],
   "source": [
    "rng = MersenneTwister(6354789)\n",
    "@time [onepars(rng, pars) for i in 1:10_000];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0.005245 seconds (10.02 k allocations: 1.146 MiB)\n"
     ]
    }
   ],
   "source": [
    "rng = MersenneTwister(6354789)\n",
    "@time simslr(rng, pars, 10_000);"
   ]
  },
  {
   "source": [
    "The optimized version is over 10 times faster and requires much less memory and fewer allocations than the original version.\n",
    "\n",
    "The savings in memory allocations and usage come from the fact that all of the memory to be used, except for the storage of the result itself, is allocated before entering the simulation loop and re-used within the loop.\n",
    "Unlike R, Julia allows a function to modify its arguments if they are composite structures like vectors.\n",
    "It is customary to append \"!\" to the names of such _mutating_ functions, like `lmul!` (multiply, in-place, on the left) and `ldiv!` (divide, in-place, on the left).\n",
    "\n",
    "Also, in the simulation loop itself, the vector `y` is filled in with the fixed part of the response plus the random noise in an inner loop.\n",
    "This is the exact opposite of the programming style favored in R and Python where loops are avoided at all cost.\n",
    "In Julia a loop is a perfectly fine way of programming an iterative operation; there is no need to perform contorsions to \"vectorize\" operations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}